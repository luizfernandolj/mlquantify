mlquantify.evaluation.protocol._Protocol
========================================

.. py:module:: mlquantify.evaluation.protocol._Protocol


Classes
-------

.. autoapisummary::

   mlquantify.evaluation.protocol._Protocol.Protocol


Module Contents
---------------

.. py:class:: Protocol(models: Union[List[Union[str, mlquantify.base.Quantifier]], str, mlquantify.base.Quantifier], batch_size: Union[List[int], int], learner: sklearn.base.BaseEstimator = None, n_iterations: int = 1, n_jobs: int = 1, random_state: int = 32, verbose: bool = False, return_type: str = 'predictions', measures: List[str] = None)

   Bases: :py:obj:`abc.ABC`


   Base class for implementing different quantification protocols.

   This abstract class provides a structure for creating protocols that involve
   fitting quantification models to training data and generating predictions on test data.
   It supports parallel processing, multiple iterations, and different output formats.

   Args:
       models (Union[List[Union[str, Quantifier]], str, Quantifier]): 
           List of quantification models, a single model name, or 'all' for all models.
       batch_size (Union[List[int], int]): 
           Size of the batches to be processed, or a list of sizes.
       learner (BaseEstimator, optional): 
           Machine learning model to be used with the quantifiers. Required for model methods.
       n_iterations (int, optional): 
           Number of iterations for the protocol. Default is 1.
       n_jobs (int, optional): 
           Number of jobs to run in parallel. Default is 1.
       random_state (int, optional): 
           Seed for random number generation. Default is 32.
       verbose (bool, optional): 
           Whether to print progress messages. Default is False.
       return_type (str, optional): 
           Type of return value ('predictions' or 'table'). Default is 'predictions'.
       measures (List[str], optional): 
           List of error measures to calculate. Must be in MEASURES or None. Default is None.


   .. py:attribute:: models


   .. py:attribute:: learner


   .. py:attribute:: batch_size


   .. py:attribute:: n_iterations


   .. py:attribute:: n_jobs


   .. py:attribute:: random_state


   .. py:attribute:: verbose


   .. py:attribute:: return_type


   .. py:attribute:: measures


   .. py:method:: _initialize_models(models, learner)


   .. py:method:: sout(msg)


   .. py:method:: fit(X_train, y_train)

      Fit all methods into the training data.

      Args:
          X_train (array-like): Features of training.
          y_train (array-like): Labels of training.



   .. py:method:: predict(X_test, y_test) -> numpy.any

      Generate several samples with artificial prevalences, and sizes. 
      And for each method, predicts with this sample, aggregating all toguether
      with a pandas dataframe if request, or else just the predictions.

      Args:
          X_test (array-like): Features of test.
          y_test (array-like): Labels of test.

      Returns:
          tuple: tuple containing the model, real_prev and pred_prev, or.
          DataFrame: table of results, along with error measures if requested. 



   .. py:method:: predict_protocol() -> numpy.ndarray
      :abstractmethod:


      Abstract method that every protocol has to implement 



   .. py:method:: _new_sample() -> tuple
      :abstractmethod:


      Abstract method of sample extraction for each protocol

      Returns:
          tuple: tuple containing the X_sample and the y_sample



   .. py:method:: _delayed_predict(args) -> tuple
      :abstractmethod:


      abstract method for predicting in the extracted
      samples, is delayed for running in parallel for 
      eficciency purposes.



   .. py:method:: _delayed_fit(args)


