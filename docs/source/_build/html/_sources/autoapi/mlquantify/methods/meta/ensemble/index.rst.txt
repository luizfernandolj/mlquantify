mlquantify.methods.meta.ensemble
================================

.. py:module:: mlquantify.methods.meta.ensemble


Classes
-------

.. autoapisummary::

   mlquantify.methods.meta.ensemble.Ensemble


Functions
---------

.. autoapisummary::

   mlquantify.methods.meta.ensemble._select_k
   mlquantify.methods.meta.ensemble._delayed_new_sample
   mlquantify.methods.meta.ensemble._delayed_predict
   mlquantify.methods.meta.ensemble._draw_simplex


Module Contents
---------------

.. py:class:: Ensemble(quantifier: mlquantify.base.Quantifier, size: int = 50, min_prop: float = 0.1, selection_metric: str = 'all', p_metric: float = 0.25, return_type: str = 'mean', max_sample_size: int = None, max_trials: int = 100, n_jobs: int = 1, verbose: bool = False)

   Bases: :py:obj:`mlquantify.base.Quantifier`


   Abstract Class for quantifiers.


   .. py:attribute:: SELECTION_METRICS

      Ensemble method, based on the articles:
      Pérez-Gállego, P., Quevedo, J. R., & del Coz, J. J. (2017).
      Using ensembles for problems with characterizable changes in data distribution: A case study on quantification.
      Information Fusion, 34, 87-100.
      and
      Pérez-Gállego, P., Castano, A., Quevedo, J. R., & del Coz, J. J. (2019). 
      Dynamic ensemble selection for quantification tasks. 
      Information Fusion, 45, 1-15.

          This approach of Ensemble is made of taking multiple
      samples varying class proportions on each, and for the 
      predictions, it takes the k models which as the minimum
      seletion metric, which are:
       - all -> return all the predictions
       - ptr -> computes the selected error measure
       - ds -> computes the hellinger distance of the train and test
       distributions for each model




   .. py:attribute:: base_quantifier


   .. py:attribute:: size


   .. py:attribute:: min_prop


   .. py:attribute:: p_metric


   .. py:attribute:: selection_metric


   .. py:attribute:: return_type


   .. py:attribute:: n_jobs


   .. py:attribute:: proba_generator
      :value: None



   .. py:attribute:: verbose


   .. py:attribute:: max_sample_size


   .. py:attribute:: max_trials


   .. py:method:: sout(msg)


   .. py:method:: fit(X, y)


   .. py:method:: predict(X)


   .. py:method:: ptr_selection_metric(prevalences)

      Selects the prevalences made by models that have been trained on samples with a prevalence that is most similar
      to a first approximation of the test prevalence as made by all models in the ensemble.



   .. py:method:: ds_get_posteriors(X, y)

      In the original article, this procedure is not described in a sufficient level of detail. The paper only says
      that the distribution of posterior probabilities from training and test examples is compared by means of the
      Hellinger Distance. However, how these posterior probabilities are generated is not specified. In the article,
      a Logistic Regressor (LR) is used as the classifier device and that could be used for this purpose. However, in
      general, a Quantifier is not necessarily an instance of Aggreggative Probabilistic Quantifiers, and so, that the
      quantifier builds on top of a probabilistic classifier cannot be given for granted. Additionally, it would not
      be correct to generate the posterior probabilities for training documents that have concurred in training the
      classifier that generates them.
      This function thus generates the posterior probabilities for all training documents in a cross-validation way,
      using a LR with hyperparameters that have previously been optimized via grid search in 5FCV.
      :return P,f, where P is a ndarray containing the posterior probabilities of the training data, generated via
      cross-validation and using an optimized LR, and the function to be used in order to generate posterior
      probabilities for test X.



   .. py:method:: ds_selection_metric(prevalences, test)


.. py:function:: _select_k(elements, order, k)

.. py:function:: _delayed_new_sample(args)

.. py:function:: _delayed_predict(args)

.. py:function:: _draw_simplex(ndim, min_val, max_trials=100)

   returns a uniform sampling from the ndim-dimensional simplex but guarantees that all dimensions
   are >= min_class_prev (for min_val>0, this makes the sampling not truly uniform)
   :param ndim: number of dimensions of the simplex
   :param min_val: minimum class prevalence allowed. If less than 1/ndim a ValueError will be throw since
   there is no possible solution.
   :return: a sample from the ndim-dimensional simplex that is uniform in S(ndim)-R where S(ndim) is the simplex
   and R is the simplex subset containing dimensions lower than min_val


