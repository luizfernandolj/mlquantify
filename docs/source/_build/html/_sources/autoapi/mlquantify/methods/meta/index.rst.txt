mlquantify.methods.meta
=======================

.. py:module:: mlquantify.methods.meta


Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/mlquantify/methods/meta/ensemble/index


Classes
-------

.. autoapisummary::

   mlquantify.methods.meta.Ensemble


Package Contents
----------------

.. py:class:: Ensemble(quantifier: mlquantify.base.Quantifier, size: int = 50, min_prop: float = 0.1, selection_metric: str = 'all', p_metric: float = 0.25, return_type: str = 'mean', max_sample_size: int = None, max_trials: int = 100, n_jobs: int = 1, verbose: bool = False)

   Bases: :py:obj:`mlquantify.base.Quantifier`


   Abstract Class for quantifiers.


   .. py:attribute:: SELECTION_METRICS

      Ensemble method, based on the articles:
      Pérez-Gállego, P., Quevedo, J. R., & del Coz, J. J. (2017).
      Using ensembles for problems with characterizable changes in data distribution: A case study on quantification.
      Information Fusion, 34, 87-100.
      and
      Pérez-Gállego, P., Castano, A., Quevedo, J. R., & del Coz, J. J. (2019). 
      Dynamic ensemble selection for quantification tasks. 
      Information Fusion, 45, 1-15.

          This approach of Ensemble is made of taking multiple
      samples varying class proportions on each, and for the 
      predictions, it takes the k models which as the minimum
      seletion metric, which are:
       - all -> return all the predictions
       - ptr -> computes the selected error measure
       - ds -> computes the hellinger distance of the train and test
       distributions for each model




   .. py:attribute:: base_quantifier


   .. py:attribute:: size


   .. py:attribute:: min_prop


   .. py:attribute:: p_metric


   .. py:attribute:: selection_metric


   .. py:attribute:: return_type


   .. py:attribute:: n_jobs


   .. py:attribute:: proba_generator
      :value: None



   .. py:attribute:: verbose


   .. py:attribute:: max_sample_size


   .. py:attribute:: max_trials


   .. py:method:: sout(msg)


   .. py:method:: fit(X, y)


   .. py:method:: predict(X)


   .. py:method:: ptr_selection_metric(prevalences)

      Selects the prevalences made by models that have been trained on samples with a prevalence that is most similar
      to a first approximation of the test prevalence as made by all models in the ensemble.



   .. py:method:: ds_get_posteriors(X, y)

      In the original article, this procedure is not described in a sufficient level of detail. The paper only says
      that the distribution of posterior probabilities from training and test examples is compared by means of the
      Hellinger Distance. However, how these posterior probabilities are generated is not specified. In the article,
      a Logistic Regressor (LR) is used as the classifier device and that could be used for this purpose. However, in
      general, a Quantifier is not necessarily an instance of Aggreggative Probabilistic Quantifiers, and so, that the
      quantifier builds on top of a probabilistic classifier cannot be given for granted. Additionally, it would not
      be correct to generate the posterior probabilities for training documents that have concurred in training the
      classifier that generates them.
      This function thus generates the posterior probabilities for all training documents in a cross-validation way,
      using a LR with hyperparameters that have previously been optimized via grid search in 5FCV.
      :return P,f, where P is a ndarray containing the posterior probabilities of the training data, generated via
      cross-validation and using an optimized LR, and the function to be used in order to generate posterior
      probabilities for test X.



   .. py:method:: ds_selection_metric(prevalences, test)


