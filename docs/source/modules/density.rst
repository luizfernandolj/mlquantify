.. _density:

.. currentmodule:: mlquantify.neighbors

=========================
Kernel Density Estimation
=========================

KDEy: Kernel Density Estimation y-Similarity
============================================

**KDEy** is a multi-class quantification approach based on Mixture Models (Distribution Matching - DM) that proposes a novel representation mechanism to model the distribution of posterior probabilities (y-scores) generated by a soft classifier [1]_.

**The Limitations of Histograms**
The core argument of KDEy is that traditional DM approaches relying on histograms are suboptimal in multi-class scenarios. Histograms tend to become specific to each class, losing the opportunity to model **inter-class interactions** that may exist in the data. Furthermore, binning strategies can lose critical information regarding the shape of the distribution [1]_.

**The KDEy Solution**
KDEy resolves this by replacing discrete, univariate histograms with **continuous, multivariate Probability Density Functions (PDFs)** modeled via Kernel Density Estimation (KDE). These PDFs are represented as Gaussian Mixture Models (GMMs) operating on the unit simplex (:math:`\Delta_{n-1}`), effectively preserving inter-class correlations [1]_.

KDEy serves as a prefix for variants that fall under different optimization frameworks.

Variants of KDEy
----------------

The literature distinguishes three main variants of KDEy, depending on the optimization framework and the divergence function utilized [1]_:

KDEy-HD (Hellinger Distance)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This variant operates within the **Distribution Matching (DM)** framework.

* **Dissimilarity:** It uses the **Hellinger Distance (HD)** as the divergence measure, seeking to minimize the discrepancy between the weighted mixture distribution and the test distribution [1]_.
* **Mechanism:** Optimization in this context typically requires Monte Carlo sampling to approximate the distance integrals.
* **Performance:** It has proven to be one of the strongest methods. In direct comparisons, KDEy-HD consistently outperforms DM-HD (its histogram-based competitor), suggesting that the KDE-based representation is inherently superior for multi-class quantification [1]_.

.. dropdown:: Mathematical details - KDEy-HD

   The objective is to minimize the Hellinger Distance between the mixture :math:`p_\alpha` and the test distribution :math:`q_U`.

   .. math::

      \hat{\alpha} = \operatorname*{arg\,min}_{\alpha \in \Delta_{n-1}} HD(p_\alpha, q_U)

   where the Hellinger distance is defined via the integral of the square root difference of the densities.

KDEy-CS (Cauchy-Schwarz)
~~~~~~~~~~~~~~~~~~~~~~~~

This variant also operates within the **Distribution Matching (DM)** framework but prioritizes computational efficiency.

* **Dissimilarity:** It uses the **Cauchy-Schwarz Divergence (DCS)**.
* **Mechanism:** This variant is particularly notable for its efficiency. Unlike other metrics, the Cauchy-Schwarz divergence does not involve log-sum terms, allowing for a **closed-form solution** [1]_. This means that computationally expensive training tensors can be pre-calculated during the training phase, significantly speeding up the test phase.
* **Performance:** KDEy-CS generally outperforms its histogram-based counterpart, DM-CS [1]_.

.. dropdown:: Mathematical details - KDEy-CS

   The Cauchy-Schwarz divergence allows the optimization problem to be solved without iterative approximation, relying on the interactions between the kernel components of the mixture models.

KDEy-ML (Maximum Likelihood)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This variant falls under the **Maximum Likelihood (ML)** framework.

* **Mechanism:** Optimizing the distribution fit in terms of Kullback-Leibler Divergence (KLD) is mathematically equivalent to maximizing the likelihood of the test data. KDEy-ML uses standard optimization routines to directly maximize the likelihood of a mixture of KDEs on the simplex [1]_.
* **Relation to EMQ:** While EMQ (SLD) also derives from the ML framework, it uses an iterative process (E-steps and M-steps). In contrast, KDEy-ML approaches the problem through **direct optimization** of the KDE mixture [1]_.
* **Performance:** KDEy-ML is highly competitive and has been shown to outperform EMQ in many scenarios, challenging a method long considered "hard-to-beat" in Label Shift literature [1]_.

**Example**

.. code-block:: python

   from mlquantify.neighbors import KDEyML
   from sklearn.ensemble import RandomForestClassifier

   # KDEy-ML uses Maximum Likelihood optimization on KDE representations
   q = KDEyML(learner=RandomForestClassifier(), bandwidth=0.1)
   q.fit(X_train, y_train)
   q.predict(X_test)

.. dropdown:: References

    .. [1] Moreo, A., Gonz√°lez, P., & del Coz, J. J. (2024). Kernel Density Estimation for Multiclass Quantification. http://arxiv.org/abs/2401.00490